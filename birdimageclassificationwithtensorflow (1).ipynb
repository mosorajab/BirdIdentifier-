{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5468571,"sourceType":"datasetVersion","datasetId":534640}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Mohamed Sohail Rajab\n\n--------------\n\nLINK TO DATASE: Gerry (2023). BIRDS 525 SPECIES- IMAGE CLASSIFICATION. [online] Kaggle.com. Available at: https://www.kaggle.com/datasets/gpiosenka/100-bird-species/data?select=birds.csv\n\n---------------\n### Welcome to this exploration of bird species classification using state-of-the-art deep learning techniques. \n\n----------\n\nThe dataset we're looking at, which contains photos of 525 different bird species, is ideal for image classification with TensorFlow for several reasons:\n\n1. **Large and Diverse Dataset**: The dataset contains 84,635 training photos and a total of 90,000 images across training, test, and validation sets. This variety and volume are essential for building robust deep learning models.\n\n\n2. **Clean and Preprocessed Data**: The dataset has been cleaned to remove duplicates and low-quality photos, allowing the model to be trained on high-quality, unique data. This stage is critical to preventing overfitting and ensuring that the model learns to generalise from a wide range of photos.\n\n\n4. **Structured Data Organization**: The dataset's structure, with separate subdirectories for each bird species in training, test, and validation sets, aligns well with TensorFlow's ImageDataGenerator.flow_from_directory method. This method is a convenient way to load and preprocess images for training with Keras models in TensorFlow.\n\n5. **Appropriate Image Resolution**: The images are standardized to a resolution of 224x224x3, which is suitable for most convolutional neural network (CNN) architectures. This uniformity in size allows for consistent input to the model without the need for additional resizing during preprocessing.\n\n6. **Suitable for Transfer Learning**: The dataset's characteristics make it ideal for transfer learning using pre-trained models available in TensorFlow (like ResNet, VGG, EfficientNet). These models, pre-trained on large datasets like ImageNet, can be fine-tuned on this bird species dataset, potentially leading to high accuracy due to the similarity in the data (natural images of objects).\n\n7. **Challenging Real-world Scenario**: The dataset reflects a real-world challenge due to the variation in bird species appearance and the imbalance in the representation of male and female species. This aspect makes it a good case for developing robust image classification models.\n\n8. **Potential for High Accuracy**: Given the dataset's quality and diversity, even moderately complex models are expected to achieve high training and test accuracies, as mentioned in the dataset description. This indicates that the dataset is well-prepared for deep learning applications.\n\n9. **Availability of Metadata**: The inclusion of a CSV file with labels, scientific names, and dataset partitions (train, test, validation) provides valuable metadata that can be used for more in-depth analysis and understanding of the model's performance.\n\nIn summary, this dataset's size, quality, and organization make it a prime candidate for training effective image classification models using TensorFlow, particularly those employing CNN architectures or transfer learning techniques.\n\n-----------------\n\n- **Exploratory Analysis and Data Wrangling**:  In order to understand the properties of the bird species image dataset, I will first undertake exploratory data analysis (EDA). This includes visualising the distribution of classes (bird species), detecting imbalances in the dataset, and assessing image quality. Data wrangling may entail organising the data into a structure suitable for TensorFlow training, such as organising photographs into folders by class or preprocessing them to a uniform size and format.\n\n- **Preparing the Environment**: The notebook contains cells for installing required programmes such as keras-preprocessing and downloading assistance functions. These steps are essential for getting the Python environment ready for data processing and model training.\n\n- **Configuration Parameters**: It sets certain parameters like BATCH_SIZE and TARGET_SIZE, which are likely used for image processing and feeding into the neural network.\n\n- **Exploratory Analysis and Data Wrangling**: The notebook likely includes steps to explore and preprocess the dataset. This would involve examining the images of birds, understanding their distribution across different classes (species), and preparing the data for model training (resizing images, normalization, etc.).\n\n- **Information Conveyed by Tables or Graphs**: Any tables or graphs in the notebook would likely show the distribution of data (like the number of images per species), model performance metrics (accuracy, loss over epochs), and possibly visualizations of the model's predictions.\n\n- **Dealing with Overfitting and Underfitting**: To address these issues, the notebook might employ techniques like data augmentation (to increase the diversity of the training set), dropout layers in the neural network (to prevent over-reliance on certain neurons), and early stopping (to halt training before the model overfits). \n\n- **Methods**: We will be making use of nerural networks, more specifically CNNs, we will be using the pre-trained model 'EfficientNetB0'.\n\n- **Hyperparameter Tuning**: This could involve adjusting parameters like learning rate, batch size, number of layers, and neurons in each layer to improve model performance. The notebook might use a systematic approach like grid search or random search to find the best combination of hyperparameters.\n\n- **Interpreting Results**: The analysis of model scores would involve looking at accuracy, precision, recall, and F1 scores. The confusion matrix would provide insights into which species are correctly classified and which are commonly confused. The effects of hyperparameter tuning on these metrics would also be examined to understand the model's performance improvements.\n\n\n\n----------------------\n","metadata":{}},{"cell_type":"code","source":"!pip install keras-preprocessing","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:41:45.530839Z","iopub.execute_input":"2023-11-21T12:41:45.531232Z","iopub.status.idle":"2023-11-21T12:41:57.169078Z","shell.execute_reply.started":"2023-11-21T12:41:45.531201Z","shell.execute_reply":"2023-11-21T12:41:57.167983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras import layers,models\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping,ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom pathlib import Path\nimport os.path\nimport random\nimport matplotlib.cm as cm\nimport cv2\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:41:57.172068Z","iopub.execute_input":"2023-11-21T12:41:57.172890Z","iopub.status.idle":"2023-11-21T12:41:57.183254Z","shell.execute_reply.started":"2023-11-21T12:41:57.172845Z","shell.execute_reply":"2023-11-21T12:41:57.182258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\nfrom helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir, pred_and_plot","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:41:57.184322Z","iopub.execute_input":"2023-11-21T12:41:57.184608Z","iopub.status.idle":"2023-11-21T12:41:58.414998Z","shell.execute_reply.started":"2023-11-21T12:41:57.184582Z","shell.execute_reply":"2023-11-21T12:41:58.413855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\nTARGET_SIZE = (224, 224)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:41:58.416581Z","iopub.execute_input":"2023-11-21T12:41:58.416912Z","iopub.status.idle":"2023-11-21T12:41:58.421827Z","shell.execute_reply.started":"2023-11-21T12:41:58.416879Z","shell.execute_reply":"2023-11-21T12:41:58.420887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading up the dataset","metadata":{}},{"cell_type":"code","source":"dataset = \"../input/100-bird-species/train\"\nwalk_through_dir(dataset); #to get better understanding of dataset structure ","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:41:58.425651Z","iopub.execute_input":"2023-11-21T12:41:58.426225Z","iopub.status.idle":"2023-11-21T12:41:58.989817Z","shell.execute_reply.started":"2023-11-21T12:41:58.426198Z","shell.execute_reply":"2023-11-21T12:41:58.988898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_dir = Path(dataset)\n\nfilepaths = list(image_dir.glob(r'**/*.JPG')) + list(image_dir.glob(r'**/*.jpg')) + list(image_dir.glob(r'**/*.png')) + list(image_dir.glob(r'**/*.png'))\n\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n\nfilepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\nimage_df = pd.concat([filepaths, labels], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:41:58.991116Z","iopub.execute_input":"2023-11-21T12:41:58.991516Z","iopub.status.idle":"2023-11-21T12:42:02.696112Z","shell.execute_reply.started":"2023-11-21T12:41:58.991478Z","shell.execute_reply":"2023-11-21T12:42:02.695073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizations ","metadata":{}},{"cell_type":"code","source":"label_counts = image_df['Label'].value_counts()[:20]\n\nplt.figure(figsize=(20, 6))\nsns.barplot(x=label_counts.index, y=label_counts.values, alpha=0.8)\nplt.title('Distribution of Top 20 Labels in Image Dataset', fontsize=16)\nplt.xlabel('Label', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:42:02.697297Z","iopub.execute_input":"2023-11-21T12:42:02.697614Z","iopub.status.idle":"2023-11-21T12:42:03.291388Z","shell.execute_reply.started":"2023-11-21T12:42:02.697587Z","shell.execute_reply":"2023-11-21T12:42:03.290485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset\nbirds_df = pd.read_csv('../input/100-bird-species/birds.csv')\n\n# Class Balance in Training Set\nplt.figure(figsize=(10, 300))  # Increase the figure size accordingly\nsns.countplot(y='labels', data=birds_df[birds_df['data set'] == 'train'], order=birds_df['labels'].value_counts().index)\nplt.title('Class Distribution in Training Set')\nplt.xlabel('Count')\nplt.ylabel('Labels')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:42:03.292581Z","iopub.execute_input":"2023-11-21T12:42:03.292852Z","iopub.status.idle":"2023-11-21T12:42:10.012662Z","shell.execute_reply.started":"2023-11-21T12:42:03.292828Z","shell.execute_reply":"2023-11-21T12:42:10.011382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparison of Dataset Splits\ndataset_counts = birds_df['data set'].value_counts()\nplt.figure(figsize=(10, 6))\nsns.barplot(x=dataset_counts.index, y=dataset_counts.values, alpha=0.8)\nplt.title('Comparison of Dataset Splits')\nplt.ylabel('Number of Images')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:42:10.013997Z","iopub.execute_input":"2023-11-21T12:42:10.014310Z","iopub.status.idle":"2023-11-21T12:42:10.315012Z","shell.execute_reply.started":"2023-11-21T12:42:10.014282Z","shell.execute_reply":"2023-11-21T12:42:10.313901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word Cloud of Species Names\nfrom wordcloud import WordCloud\n\n# Generating the word cloud data\nwordcloud_data = birds_df['labels'].str.cat(sep=' ')\nwordcloud = WordCloud(width=800, height=400, background_color ='white').generate(wordcloud_data)\n\n# Displaying the word cloud\nplt.figure(figsize=(15, 8))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Word Cloud of Species Names')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:42:10.316245Z","iopub.execute_input":"2023-11-21T12:42:10.316552Z","iopub.status.idle":"2023-11-21T12:42:12.338993Z","shell.execute_reply.started":"2023-11-21T12:42:10.316525Z","shell.execute_reply":"2023-11-21T12:42:12.338065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_index = np.random.randint(0, len(image_df), 16)\nfig, axes = plt.subplots(nrows=4, ncols=4, figsize=(10, 10),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(image_df.Filepath[random_index[i]]))\n    ax.set_title(image_df.Label[random_index[i]])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:42:12.340214Z","iopub.execute_input":"2023-11-21T12:42:12.340545Z","iopub.status.idle":"2023-11-21T12:42:13.974055Z","shell.execute_reply.started":"2023-11-21T12:42:12.340517Z","shell.execute_reply":"2023-11-21T12:42:13.973115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pre-processing ","metadata":{}},{"cell_type":"code","source":"# masouduut94 (2021). Forgery Detection by using extracted ELA features. \n# [online] Kaggle.com. Available at: https://www.kaggle.com/code/masouduut94/forgery-detection-by-using-extracted-ela-features\n\n\ndef compute_ela_cv(path, quality):\n    temp_filename = 'temp_file_name.jpeg'\n    SCALE = 15\n    orig_img = cv2.imread(path)\n    orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n    \n    cv2.imwrite(temp_filename, orig_img, [cv2.IMWRITE_JPEG_QUALITY, quality])\n\n    compressed_img = cv2.imread(temp_filename)\n\n    diff = SCALE * cv2.absdiff(orig_img, compressed_img)\n    return diff\n\n\ndef convert_to_ela_image(path, quality):\n    temp_filename = 'temp_file_name.jpeg'\n    ela_filename = 'temp_ela.png'\n    image = Image.open(path).convert('RGB')\n    image.save(temp_filename, 'JPEG', quality = quality)\n    temp_image = Image.open(temp_filename)\n\n    ela_image = ImageChops.difference(image, temp_image)\n\n    extrema = ela_image.getextrema()\n    max_diff = max([ex[1] for ex in extrema])\n    if max_diff == 0:\n        max_diff = 1\n\n    scale = 255.0 / max_diff\n    ela_image = ImageEnhance.Brightness(ela_image).enhance(scale)\n    \n    return ela_image\n\n\ndef random_sample(path, extension=None):\n    if extension:\n        items = Path(path).glob(f'*.{extension}')\n    else:\n        items = Path(path).glob(f'*')\n        \n    items = list(items)\n        \n    p = random.choice(items)\n    return p.as_posix()","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:42:13.975395Z","iopub.execute_input":"2023-11-21T12:42:13.975734Z","iopub.status.idle":"2023-11-21T12:42:13.985832Z","shell.execute_reply.started":"2023-11-21T12:42:13.975705Z","shell.execute_reply":"2023-11-21T12:42:13.984931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p = random_sample('../input/100-bird-species/train/SCARLET MACAW')\norig = cv2.imread(p)\norig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB) / 255.0\ninit_val = 100\ncolumns = 3\nrows = 3\n\nfig=plt.figure(figsize=(15, 10))\nfor i in range(1, columns*rows +1):\n    quality=init_val - (i-1) * 8\n    img = compute_ela_cv(path=p, quality=quality)\n    if i == 1:\n        img = orig.copy()\n    ax = fig.add_subplot(rows, columns, i) \n    ax.title.set_text(f'q: {quality}')\n    plt.imshow(img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:42:13.986988Z","iopub.execute_input":"2023-11-21T12:42:13.987259Z","iopub.status.idle":"2023-11-21T12:42:16.212718Z","shell.execute_reply.started":"2023-11-21T12:42:13.987235Z","shell.execute_reply":"2023-11-21T12:42:16.211718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = train_test_split(image_df, test_size=0.2, shuffle=True, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:42:16.216613Z","iopub.execute_input":"2023-11-21T12:42:16.216934Z","iopub.status.idle":"2023-11-21T12:42:16.243065Z","shell.execute_reply.started":"2023-11-21T12:42:16.216906Z","shell.execute_reply":"2023-11-21T12:42:16.242123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n    validation_split=0.2\n)\n\ntest_generator = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:42:16.244278Z","iopub.execute_input":"2023-11-21T12:42:16.244585Z","iopub.status.idle":"2023-11-21T12:42:16.249586Z","shell.execute_reply.started":"2023-11-21T12:42:16.244559Z","shell.execute_reply":"2023-11-21T12:42:16.248601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=TARGET_SIZE,\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=42,\n    subset='training'\n)\n\nval_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=TARGET_SIZE,\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=42,\n    subset='validation'\n)\n\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=TARGET_SIZE,\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:42:16.250748Z","iopub.execute_input":"2023-11-21T12:42:16.251011Z","iopub.status.idle":"2023-11-21T12:43:01.314351Z","shell.execute_reply.started":"2023-11-21T12:42:16.250987Z","shell.execute_reply":"2023-11-21T12:43:01.313229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Team, K. (2020). Keras documentation: Working with preprocessing layers. \n#[online] Keras.io. Available at: https://keras.io/guides/preprocessing_layers/ \n\naugment = tf.keras.Sequential([\n  layers.experimental.preprocessing.Resizing(224,224),\n  layers.experimental.preprocessing.Rescaling(1./255),\n  layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n  layers.experimental.preprocessing.RandomRotation(0.1),\n  layers.experimental.preprocessing.RandomZoom(0.1),\n  layers.experimental.preprocessing.RandomContrast(0.1),\n])","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:43:01.317908Z","iopub.execute_input":"2023-11-21T12:43:01.318200Z","iopub.status.idle":"2023-11-21T12:43:01.338029Z","shell.execute_reply.started":"2023-11-21T12:43:01.318173Z","shell.execute_reply":"2023-11-21T12:43:01.337168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Team, K. (2020). Keras documentation: Image classification via fine-tuning with EfficientNet. \n#[online] Keras.io. Available at: https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/\n#TRANSFER LEARNING\n\npretrained_model = tf.keras.applications.efficientnet.EfficientNetB0(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='max'\n)\n\npretrained_model.trainable = False\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T15:20:37.266119Z","iopub.execute_input":"2023-11-21T15:20:37.266891Z","iopub.status.idle":"2023-11-21T15:20:39.243110Z","shell.execute_reply.started":"2023-11-21T15:20:37.266855Z","shell.execute_reply":"2023-11-21T15:20:39.242242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Team, K. (2023). Keras documentation: EarlyStopping. \n#[online] Keras.io. Available at: https://keras.io/api/callbacks/early_stopping/\n\ncheckpoint_path = \"birds_classification_model_checkpoint\"\ncheckpoint_callback = ModelCheckpoint(checkpoint_path,\n                                      save_weights_only=True,\n                                      monitor=\"val_accuracy\",\n                                      save_best_only=True)\n\nearly_stopping = EarlyStopping(monitor = \"val_loss\",\n                               patience = 5,\n                               restore_best_weights = True) \n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:43:03.315075Z","iopub.execute_input":"2023-11-21T12:43:03.315355Z","iopub.status.idle":"2023-11-21T12:43:03.321325Z","shell.execute_reply.started":"2023-11-21T12:43:03.315330Z","shell.execute_reply":"2023-11-21T12:43:03.320385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN and Pipeline ","metadata":{}},{"cell_type":"code","source":"#Image classification (2023). Image classification. [online] TensorFlow. Available at: \n#https://www.tensorflow.org/tutorials/images/classification\n#PIPELINE\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\nfrom tensorflow.keras import layers\n\n\n# Define the input layer\ninputs = pretrained_model.input\n\ndata_augmentation = tf.keras.Sequential([\n    layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n    layers.experimental.preprocessing.RandomRotation(0.1),\n    layers.experimental.preprocessing.RandomZoom(0.2),\n])\n\nx = data_augmentation(inputs)\n\n\nx = pretrained_model(x, training=True)\n\nx = Flatten()(x) \nx = Dense(128, activation='relu')(x)\nx = Dropout(0.45)(x)\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.45)(x)\n\n# Output layer\noutputs = Dense(525, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Compile \nmodel.compile(\n    optimizer=Adam(0.0001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5)\ncheckpoint_callback = ModelCheckpoint('model_checkpoint.h5', save_best_only=True)\ntensorboard_callback = TensorBoard(log_dir='training_logs')\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)\n\n# Train the model\nhistory = model.fit(\n    train_images,  \n    steps_per_epoch=len(train_images),\n    validation_data=val_images,  \n    validation_steps=len(val_images),\n    epochs=80,\n    callbacks=[\n        early_stopping,\n        tensorboard_callback,\n        checkpoint_callback,\n        reduce_lr\n    ]\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T12:43:03.322641Z","iopub.execute_input":"2023-11-21T12:43:03.322889Z","iopub.status.idle":"2023-11-21T15:10:57.120398Z","shell.execute_reply.started":"2023-11-21T12:43:03.322867Z","shell.execute_reply":"2023-11-21T15:10:57.119381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation ","metadata":{}},{"cell_type":"code","source":"results = model.evaluate(test_images, verbose=0)\nprint(\"Test Loss: {:.5f}\".format(results[0]))\nprint(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2023-11-21T15:10:57.121913Z","iopub.execute_input":"2023-11-21T15:10:57.122712Z","iopub.status.idle":"2023-11-21T15:11:45.325132Z","shell.execute_reply.started":"2023-11-21T15:10:57.122681Z","shell.execute_reply":"2023-11-21T15:11:45.324203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing loss","metadata":{}},{"cell_type":"code","source":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'b', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')\n\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\n\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-21T15:11:45.326513Z","iopub.execute_input":"2023-11-21T15:11:45.326822Z","iopub.status.idle":"2023-11-21T15:11:46.030639Z","shell.execute_reply.started":"2023-11-21T15:11:45.326794Z","shell.execute_reply":"2023-11-21T15:11:46.029582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Making predictions on the Test Data","metadata":{}},{"cell_type":"code","source":"pred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\nprint(f'The first 5 predictions: {pred[:5]}')","metadata":{"execution":{"iopub.status.busy":"2023-11-21T15:11:46.032042Z","iopub.execute_input":"2023-11-21T15:11:46.032431Z","iopub.status.idle":"2023-11-21T15:12:22.321652Z","shell.execute_reply.started":"2023-11-21T15:11:46.032395Z","shell.execute_reply":"2023-11-21T15:12:22.320606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PREDICTION TIME !!!","metadata":{}},{"cell_type":"code","source":"random_index = np.random.randint(0, len(test_df) - 1, 15)\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(25, 15),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df.Filepath.iloc[random_index[i]]))\n    if test_df.Label.iloc[random_index[i]] == pred[random_index[i]]:\n        color = \"green\"\n    else:\n        color = \"red\"\n    ax.set_title(f\"True: {test_df.Label.iloc[random_index[i]]}\\nPredicted: {pred[random_index[i]]}\", color=color)\nplt.show()\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-11-21T15:25:30.388117Z","iopub.execute_input":"2023-11-21T15:25:30.388537Z","iopub.status.idle":"2023-11-21T15:25:32.486266Z","shell.execute_reply.started":"2023-11-21T15:25:30.388502Z","shell.execute_reply":"2023-11-21T15:25:32.485432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the Classification Reports and Confusion Matrix","metadata":{}},{"cell_type":"code","source":"y_test = list(test_df.Label)\nprint(classification_report(y_test, pred))","metadata":{"execution":{"iopub.status.busy":"2023-11-21T15:12:24.524554Z","iopub.execute_input":"2023-11-21T15:12:24.524846Z","iopub.status.idle":"2023-11-21T15:12:24.912113Z","shell.execute_reply.started":"2023-11-21T15:12:24.524820Z","shell.execute_reply":"2023-11-21T15:12:24.911157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final Model Summary ","metadata":{}},{"cell_type":"code","source":"report = classification_report(y_test, pred, output_dict=True)\ndf = pd.DataFrame(report).transpose()\ndf","metadata":{"execution":{"iopub.status.busy":"2023-11-21T15:12:24.913561Z","iopub.execute_input":"2023-11-21T15:12:24.914201Z","iopub.status.idle":"2023-11-21T15:12:25.318036Z","shell.execute_reply.started":"2023-11-21T15:12:24.914162Z","shell.execute_reply":"2023-11-21T15:12:25.317027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see from the summary above, we are getting really good scores! \n\nsome future improvements would be incresing the epochs testing time, we can increse epochs from 80 to 200 to test for better, more stable, acuraccy.","metadata":{}}]}